{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stroke RAG Agent Notebook\n",
        "\n",
        "Sections: Environment Setup ¬∑ PDF Downloading + Processing ¬∑ Text Cleaning & Chunking ¬∑ Embedding + Vector Store Creation ¬∑ Agent Definition ¬∑ Reflection Function ¬∑ Workflow Function ¬∑ Chat Loop ¬∑ Final Instructions for Usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "# !pip install --quiet openai python-dotenv requests PyMuPDF nltk langchain-text-splitters faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temp dir: /tmp/rag_pdfs_qufsgbvy\n"
          ]
        }
      ],
      "source": [
        "import os, re, json, tempfile, pathlib, math\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import faiss        # Facebook AI Similarity Search -> library for efficient similarity search and clustering of dense vectors\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# Download stopwords quietly and load env vars/API key\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# URLs to fetch\n",
        "PDF_URLS = [\n",
        "    \"https://dnvp9c1uo2095.cloudfront.net/cms-content/030-046l_S2e_Akuttherapie-des-ischaemischen-Schlaganfalls_2022-11-verlaengert_1718363551944.pdf\",\n",
        "    \"https://dnvp9c1uo2095.cloudfront.net/cms-content/030133_LL_Sekunda%CC%88rprophylaxe_Teil_1_2022_final_korr_1739803472035.pdf\",\n",
        "    \"https://dnvp9c1uo2095.cloudfront.net/cms-content/030143_LL_Sekunda%CC%88rprophylaxe_Teil2_2022_V1.1_1670949892924.pdf\",\n",
        "    \"https://tempis.de/download/tempis-sop-2025/?tmstv=1765451597\",\n",
        "]\n",
        "\n",
        "# Controls for extraction and chunking\n",
        "START_TITLES = [\"Was gibt es Neues?\", \"Die wichtigsten Empfehlungen auf einen Blick\"]\n",
        "EXCLUDE_HEADINGS = [\"Literatur\", \"Referenzen\", \"References\", \"Bibliografie\"]\n",
        "CHUNK_SIZE = 950\n",
        "CHUNK_OVERLAP = 180\n",
        "EMBED_MODEL = \"text-embedding-3-large\"      # OpenAI‚Äôs latest embedding model, high-dimensional vectors (1536 dimensions)\n",
        "CHAT_MODEL = \"gpt-4.1-mini\"\n",
        "\n",
        "# Temp directory to store downloaded PDFs\n",
        "TEMP_DIR = pathlib.Path(tempfile.mkdtemp(prefix=\"rag_pdfs_\"))\n",
        "print(f\"Temp dir: {TEMP_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDF Downloading + Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded: /tmp/rag_pdfs_qufsgbvy/doc_1.pdf\n",
            "Downloaded: /tmp/rag_pdfs_qufsgbvy/doc_2.pdf\n",
            "Downloaded: /tmp/rag_pdfs_qufsgbvy/doc_3.pdf\n",
            "Downloaded: /tmp/rag_pdfs_qufsgbvy/doc_4.pdf\n"
          ]
        }
      ],
      "source": [
        "def download_pdfs(urls: List[str], dest_dir: pathlib.Path) -> List[pathlib.Path]:\n",
        "    \"\"\"Download all PDFs to a temp folder and return their paths.\"\"\"\n",
        "    files = []\n",
        "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for i, url in enumerate(urls, 1):\n",
        "        fname = dest_dir / f\"doc_{i}.pdf\"\n",
        "        resp = requests.get(url, timeout=60)\n",
        "        resp.raise_for_status()\n",
        "        fname.write_bytes(resp.content)\n",
        "        files.append(fname)\n",
        "        print(f\"Downloaded: {fname}\")\n",
        "    return files\n",
        "\n",
        "pdf_paths = download_pdfs(PDF_URLS, TEMP_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Cleaning & Chunking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 550234 chars from doc_1.pdf\n",
            "Extracted 261721 chars from doc_2.pdf\n",
            "Extracted 313425 chars from doc_3.pdf\n",
            "Extracted 0 chars from doc_4.pdf\n",
            "Total chunks: 1567\n"
          ]
        }
      ],
      "source": [
        "# Load stopwords (German + English) for light heuristics\n",
        "stop_words = set(stopwords.words(\"german\")) | set(stopwords.words(\"english\"))\n",
        "\n",
        "def page_contains_start(text: str) -> bool:\n",
        "    \"\"\"Detect first relevant section titles to start extraction.\"\"\"\n",
        "    return any(title.lower() in text.lower() for title in START_TITLES)\n",
        "\n",
        "def heading_is_excluded(text: str) -> bool:\n",
        "    \"\"\"Check if a heading should be skipped (references).\"\"\"\n",
        "    return any(h.lower() in text.lower() for h in EXCLUDE_HEADINGS)\n",
        "\n",
        "def looks_like_reference_block(text: str) -> bool:\n",
        "    \"\"\"Heuristic to drop pages/blocks that are mostly references.\"\"\"\n",
        "    ref_patterns = [r\"\\[[0-9]{1,3}\\]\", r\"\\([0-9]{1,3}\\)\", r\"\\d{4}\\.\"]\n",
        "    hits = sum(len(re.findall(p, text)) for p in ref_patterns)\n",
        "    density = hits / max(1, len(text.split()))\n",
        "    return density > 0.08 or heading_is_excluded(text[:80])\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Light cleaning: drop URLs, citations, page numbers, extra whitespace.\"\"\"\n",
        "    text = re.sub(r\"https?://\\S+\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d+\\s*/\\s*\\d+\\b\", \" \", text)  # page numbers like 12/34\n",
        "    text = re.sub(r\"\\s+\\d+\\s+\", \" \", text)  # lone numbers\n",
        "    text = re.sub(r\"\\[[0-9]+\\]|\\([0-9]+\\)|\\^{[0-9]+}\", \" \", text)  # citation markers\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def extract_relevant_text(pdf_path: pathlib.Path) -> str:\n",
        "    \"\"\"Extract text starting at first target section; skip reference-like pages.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    collecting = False\n",
        "    blocks = []\n",
        "    for page in doc:\n",
        "        txt = page.get_text(\"text\") or \"\"\n",
        "        if not collecting and page_contains_start(txt):\n",
        "            collecting = True\n",
        "        if not collecting:\n",
        "            continue  # skip intro pages\n",
        "        if looks_like_reference_block(txt):\n",
        "            continue  # drop reference pages/sections\n",
        "        cleaned = clean_text(txt)\n",
        "        if cleaned:\n",
        "            blocks.append(cleaned)\n",
        "    doc.close()\n",
        "    return \"\\n\".join(blocks)\n",
        "\n",
        "# Extract from all PDFs\n",
        "documents = []\n",
        "for p in pdf_paths:\n",
        "    extracted = extract_relevant_text(p)\n",
        "    documents.append(extracted)\n",
        "    print(f\"Extracted {len(extracted)} chars from {p.name}\")\n",
        "\n",
        "# Combine docs and split into overlapping chunks for retrieval\n",
        "all_text = \"\\n\".join(documents)\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \".\", \" \"]\n",
        ")\n",
        "chunks = splitter.split_text(all_text)\n",
        "print(f\"Total chunks: {len(chunks)}\")\n",
        "\n",
        "# Simple metadata mapping per chunk\n",
        "metadatas = [{\"source\": f\"chunk_{i}\"} for i in range(len(chunks))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding + Vector Store Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index ready 1567\n"
          ]
        }
      ],
      "source": [
        "def embed_texts(texts: List[str]) -> np.ndarray:\n",
        "    \"\"\"Batch-embed all chunks using OpenAI embeddings.\"\"\"\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), 64):\n",
        "        batch = texts[i:i+64]\n",
        "        resp = client.embeddings.create(model=EMBED_MODEL, input=batch)\n",
        "        embeddings.extend([item.embedding for item in resp.data])\n",
        "    return np.array(embeddings, dtype=\"float32\")\n",
        "\n",
        "# Compute embeddings and build FAISS index\n",
        "emb_matrix = embed_texts(chunks)\n",
        "index = faiss.IndexFlatL2(emb_matrix.shape[1])\n",
        "index.add(emb_matrix)\n",
        "\n",
        "# Store mappings for retrieval\n",
        "id2text = {i: t for i, t in enumerate(chunks)}\n",
        "id2meta = {i: m for i, m in enumerate(metadatas)}\n",
        "\n",
        "print(\"FAISS index ready\", index.ntotal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Persona/system prompt guiding clinician vs patient tone\n",
        "SYSTEM_PERSONA = (\n",
        "    \"You are a highly experienced neurologist specialized in stroke medicine. \"\n",
        "    \"When talking to clinicians, use precise medical terminology and cite key recommendations succinctly. \"\n",
        "    \"When talking to patients or families, simplify explanations, avoid jargon, and be empathetic. \"\n",
        "    \"Always base answers on retrieved context; if insufficient, say so and request clarification.\"\n",
        ")\n",
        "\n",
        "def retrieve(query: str, k: int = 5) -> List[Tuple[str, Dict]]:\n",
        "    \"\"\"Vector search top-k chunks for a query.\"\"\"\n",
        "    q_emb = np.array(client.embeddings.create(model=EMBED_MODEL, input=[query]).data[0].embedding, dtype=\"float32\")\n",
        "    q_emb = np.expand_dims(q_emb, axis=0)\n",
        "    scores, idxs = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], idxs[0]):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        results.append((id2text[idx], id2meta[idx]))\n",
        "    return results\n",
        "\n",
        "def build_context(chunks: List[Tuple[str, Dict]]) -> str:\n",
        "    \"\"\"Format retrieved chunks into a readable context block.\"\"\"\n",
        "    parts = []\n",
        "    for i, (text, meta) in enumerate(chunks, 1):\n",
        "        parts.append(f\"[Chunk {i}] {text}\")\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "def agent_answer(query: str, context: str) -> str:\n",
        "    \"\"\"Generate a draft answer using persona + retrieved context.\"\"\"\n",
        "    msgs = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PERSONA},\n",
        "        {\"role\": \"system\", \"content\": \"Use the provided context. If context is thin, say so and request clarification.\"},\n",
        "        {\"role\": \"system\", \"content\": f\"Context:\\n{context}\"},\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "    ]\n",
        "    resp = client.chat.completions.create(model=CHAT_MODEL, messages=msgs)\n",
        "    return resp.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conversation Memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple rolling memory of recent Q&A pairs to keep context alive\n",
        "MAX_MEMORY = 5  # keep last 5 exchanges\n",
        "memory_buffer: List[Tuple[str, str]] = []\n",
        "\n",
        "def add_to_memory(question: str, answer: str) -> None:\n",
        "    memory_buffer.append((question, answer))\n",
        "    if len(memory_buffer) > MAX_MEMORY:\n",
        "        memory_buffer.pop(0)\n",
        "\n",
        "def build_memory_context() -> str:\n",
        "    if not memory_buffer:\n",
        "        return \"\"\n",
        "    formatted = []\n",
        "    for i, (q, a) in enumerate(memory_buffer, 1):\n",
        "        formatted.append(f\"[Memory {i}] Q: {q}\\nA: {a}\")\n",
        "    return \"\\n\\n\".join(formatted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reflect_and_improve(response: str, query: str, context: str) -> str:\n",
        "    \"\"\"Check medical quality; if feedback not OK, regenerate with fixes.\"\"\"\n",
        "    reflection = client.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are checking whether the response is medically accurate, complete, safe, and well-structured. Respond with 'OK' or provide concrete corrections and missing points.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": f\"Query: {query}\\n\\nContext:\\n{context}\\n\\nDraft response:\\n{response}\"},\n",
        "        ],\n",
        "    )\n",
        "    feedback = reflection.choices[0].message.content.strip()\n",
        "    if feedback.upper() == \"OK\":\n",
        "        return response\n",
        "    # Regenerate with feedback injected\n",
        "    regen_msgs = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PERSONA},\n",
        "        {\"role\": \"system\", \"content\": \"Use the provided context. If context is thin, say so and ask for clarification.\"},\n",
        "        {\"role\": \"system\", \"content\": f\"Context:\\n{context}\"},\n",
        "        {\"role\": \"system\", \"content\": f\"Please fix issues noted: {feedback}\"},\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "    ]\n",
        "    regen = client.chat.completions.create(model=CHAT_MODEL, messages=regen_msgs)\n",
        "    return regen.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def workflow(query: str, k: int = 5) -> str:\n",
        "    \"\"\"Full RAG pipeline: retrieve ‚Üí draft ‚Üí reflect ‚Üí final (with memory).\"\"\"\n",
        "    retrieved = retrieve(query, k=k)\n",
        "    retrieval_context = build_context(retrieved)\n",
        "    memory_context = build_memory_context()\n",
        "\n",
        "    # Combine memory + retrieval context for richer answers\n",
        "    if memory_context:\n",
        "        combined_context = f\"{memory_context}\\n\\n{retrieval_context}\"\n",
        "    else:\n",
        "        combined_context = retrieval_context\n",
        "\n",
        "    draft = agent_answer(query, combined_context)\n",
        "    final = reflect_and_improve(draft, query, combined_context)\n",
        "\n",
        "    # Persist this turn in memory\n",
        "    add_to_memory(query, final)\n",
        "    return final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type 'exit' or 'quit' to stop.\n",
            "Bye!\n",
            "Conversation memory cleared.\n"
          ]
        }
      ],
      "source": [
        "print(\"Type 'exit' or 'quit' to stop.\")\n",
        "while True:\n",
        "    q = input(\"üë§: \")\n",
        "    if q.strip().lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Bye! Have a healthy day!\")\n",
        "        break\n",
        "    print(\"Checking the guidelines...\")\n",
        "    answer = workflow(q)\n",
        "    print(f\"üßë‚Äç‚öïÔ∏è: {answer}\\n\")\n",
        "\n",
        "# Reset conversation memory after the chat loop ends\n",
        "memory_buffer.clear()\n",
        "print(\"Conversation memory cleared.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Instructions for Usage\n",
        "\n",
        "- Ensure your `.env` contains `OPENAI_API_KEY` (and organization/project if required).\n",
        "- Run cells top-to-bottom. First run will download PDFs, embed, and build FAISS (may take a few minutes).\n",
        "- Use the chat loop to interact. Type `exit` or `quit` to end.\n",
        "- Persona: senior stroke neurologist; adapts to clinicians vs patients; cites context and asks for clarification when retrieval is thin.\n",
        "- Reflection: every answer is checked; if not `OK`, it regenerates with the feedback.\n",
        "- Memory: the last 5 Q&A turns are remembered and injected ahead of retrieval context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Potential improvements\n",
        "\n",
        "- add a tool which allows it to search online or on pubmed when asked about studies / trials"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
